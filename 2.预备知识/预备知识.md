# 预备知识

所有机器学习方法都涉及从数据中提取信息。 因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。

- 机器学习通常需要处理大型数据集
- 深度学习是关于优化的学习
- 机器学习还涉及如何做出预测：给定观察到的信息，某些未知属性可能的值是多少？ 要在不确定的情况下进行严格的推断，我们需要借用概率语言

这里仅仅只是最初的一个小的介绍：函数的形式和对应的代码（仅仅是打基础）

## 代码方面



### 数据操作



#### 读取数据和写入数据

导入torch。请注意，虽然它被称为PyTorch，但是代码中使用torch而不是pytorch。

```
import torch
```

张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的*向量*（vector）； 具有两个轴的张量对应数学上的*矩阵*（matrix）； 具有两个轴以上的张量没有特殊的数学名称。

首先，我们可以使用 `arange` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。

```
x = torch.arange(12)
torch.Size([12])
x.numel()
```

要想改变一个张量的形状而不改变元素数量和元素值，可以调用`view` 或者`reshape`函数

我们不需要手动指定每一个维度，我们的目标形状是（高度，宽度。在确定张量总数我们只需要确定一个就可以创建全部，即我们可以通过-1来调用此自动计算出维度的功能

```
x.view(-1,3)
x.reshape(6,-1)
```



有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0

创建一个形状为（2, 3, 4）的全零张量后，输出结果显示的确实是两个（3, 4）的全零张量。这是因为这个三维张量可以看作由两个二维（3, 4）的张量堆叠而成。在这个张量中：

- 最外层的维度大小为2，表示有两个这样的二维张量。
- 中间的维度大小为3，表示每个二维张量有3行。
- 最内层的维度大小为4，表示每行有4个元素。



创建了一个形状为（2, 3, 4）的全零张量，这意味着张量有三个维度：

- 第一个维度有2个元素，

- 第二个维度有3个元素，
- 第三个维度有4个元素。

在这种情况下，你不会得到4个2行3列的张量，而是得到一个包含2个3行4列的二维张量的三维张量。这个三维张量可以视为一个数据容器，其中有2个矩阵，每个矩阵都是3行4列。

当你在控制台打印这样的三维张量时，输出通常会按照三维结构来格式化，所以你会看到两个分隔的（3, 4）张量，而不是四个（2, 3）张量。每个（3, 4）张量是三维张量中的一个“切片”，沿着最高的维度（在这个例子中是第一个维度）切分。

打印出来的结构体现了张量的层次化组织，其中最外层的维度决定了有多少个子张量（在这个例子中是2个），而不是将所有的元素都平坦化成一系列二维张量。

<!--这不符合三维（x,y,z）的习惯啊-->

在多维数组或张量中，维度的顺序可能会根据上下文和使用的库而有所不同。在数学和物理中，我们通常将三维空间描述为 (*x*,*y*,*z*)，其中 *x* 是长度，*y* 是宽度，而 *z* 是高度或深度。然而，在多维数组的上下文中，特别是在计算机编程和数据科学中，我们通常会按照不同的顺序来描述这些维度。

在深度学习框架如 PyTorch 中，一个三维张量的形状 (2,3,4) 通常按照如下方式解释：

- 第一个维度（2）表示批量大小或堆叠的数量，可以看作是有多少个矩阵或切片。
- 第二个维度（3）通常表示行数。
- 第三个维度（4）通常表示列数。

因此，(2,3,4) 张量在 PyTorch 中表示有2个矩阵，每个矩阵有3行和4列。当你打印出这样的张量时，输出会以嵌套的方式来展示这种层次结构，显示为两个独立的 ((3,4) 矩阵。

这种表示方式与数学或物理学的习惯不完全一致，但它非常适合于编程和处理数据集，特别是在需要处理数据批次（例如，在批量处理图像时）的情况下。

```
torch.zeros((2, 3, 4))
print(torch.zeros((2, 3, 4)))
```



未初始化的张量："未初始化"意味着张量中的元素是未定义的，即它们可能包含任意值，具体值取决于内存中该位置的当前状态。

```
torch.empty((2, 3, 4))
print(torch.empty((2, 3, 4)))
```

输出：

```
tensor([[[-1.1829e+08,  4.5579e-41, -1.1829e+08,  4.5579e-41],
         [ 4.4842e-44,  0.0000e+00,  8.9683e-44,  0.0000e+00],
         [ 1.2247e-34,  0.0000e+00,  1.8037e+28,  7.5640e+23]],

        [[ 1.5793e-19,  8.7721e-43,  3.5706e+04,  1.8499e+20],
         [ 3.0221e+32,  4.8194e-32,  2.8665e+32,  1.8502e+28],
         [ 2.0831e+35,  7.4315e+28,  9.1084e-44,  0.0000e+00]]])
```

解释下：

torch.empty((2, 3, 4)) 创建了一个形状为 (2, 3, 4) 的未初始化的张量。"未初始化"意味着张量中的元素是未定义的，即它们可能包含任意值，具体值取决于内存中该位置的当前状态。

在打印该张量时，我们得到了一组随机数值，这些数值是张量被分配时内存中的现存值。这就是为什么你看到的数值可能看起来非常奇怪，它们不是0，也不是其他特定的值。使用未初始化的张量之前，通常需要将其明确地赋值或用某种方式初始化。

在这个例子中，张量的输出表明它是由两个 (3, 4) 的二维张量组成的三维张量，每个二维张量中包含了随机的、未定义的浮点数。这种张量通常在你打算立即用数据填充它，并且不在意初始值时使用



有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（1,3,4）的张量，其中所有元素都设置为0,或者全部为1

```
torch.zeros((1, 3, 4))
torch.ones((1, 3, 4))
```



#### 数学运算

**常见的运算：+，-，*，/**

这里分别进行了张量的2加减乘除运算，在这里面，可以得到我设置的一维张量的运算结果

```
x = torch.tensor([1,2,4,8])
y = torch.tensor([2,5,8,4])
x+y,x-y, x*x, y*y
print(x+y,x-y, x*x, y*y)
```



**点积和乘积运算**

在这里我使用`view`来重塑矩阵，但是得到的结果和前面出来的结果没有区别，这里的解释是：将这两个二维张量相加，确实会进行对应元素的加法，因为它们的形状是一样的。

这是因为加法是一个逐元素（element-wise）的操作，所以当您对两个形状相同的张量进行加法时，PyTorch 会将它们对应位置的元素相加。

- 逐元素乘法（Hadamard product 或 element-wise multiplication）：这时，两个形状相同的张量的对应元素相乘。在 PyTorch 中，这通过 * 运算符或者 torch.mul() 函数实现。

- 矩阵乘法（matrix multiplication）：这时，进行的是传统的矩阵乘法，其中第一个矩阵的列数必须与第二个矩阵的行数相同。在 PyTorch 中，这通过 `torch.matmul()` 或 `` @` 运算符实现。



加减乘除没有什么好说的，这里有几个好玩的`疑点`：

第一次我使用正常方式进行乘除，没有问题，就是每个值对应的变换运算

第二次我使用`view`进行重塑，本着把这个玩意想让看见矩阵乘法的运算的心态去做的时候，发现还是不行，得到的结果与上文一般无二

在我查阅一番得到的乘积有2种操作：

- 逐元素乘法（Hadamard product 或 element-wise multiplication）：这时，两个形状相同的张量的对应元素相乘。在 PyTorch 中，这通过 * 运算符或者 torch.mul() 函数实现。

- 矩阵乘法（matrix multiplication）：这时，进行的是传统的矩阵乘法，其中第一个矩阵的列数必须与第二个矩阵的行数相同。在 PyTorch 中，这通过 torch.matmul() 或 @ 运算符实现。

但是我在使用@时候出来的结果是`76`？

不是矩阵乘法吗？

在我继续查阅得到的结果是

虽然调用 x.view(-1, 2) 和 y.view(-1, 2) 将会把一维张量 x 和 y 重塑为两行两列的二维张量。

然而，这两行代码并没有改变 x 和 y 的原始值，因为 view 方法返回的是一个新的张量视图，除非这个新的视图被赋给一个新的变量或者覆盖原变量，否则原始的张量 x 和 y 保持不变。因此，如果后续没有将这些重塑后的张量赋给新变量，那么在执行 x @ y 时，仍然是对原始的一维张量 x 和 y 进行点积运算。

所以`x*y是逐元素乘法``x@y是点积（内积）运算，把逐元素乘法得到的答案给加起来`

```
x = torch.tensor([1,2,4,8])
y = torch.tensor([2,5,8,4])
x+y,x-y, x*x, y*y
print(x+y,x-y, x*x, y*y)

x.view(-1,2)
y.view(-1,2)
x.view(-1,2) + y.view(-1,2)
print(x+y,x-y, x*x, y*y,x*y,x@y)

x_reshaped = x.view(-1, 2)
y_reshaped = y.view(-1, 2)
result = x_reshaped @ y_reshaped.T  # 注意：为了进行矩阵乘法，需要转置其中一个张量
print(result)
```

`x*x` 和 `y*y` 是逐元素乘法，而不是矩阵乘法。即使 `x` 和 `y` 被重塑为二维张量，使用 `*` 运算符的结果仍然是逐元素乘法

在这里我得到的结果是：

```
tensor([ 3,  7, 12, 12]) tensor([-1, -3, -4,  4]) tensor([ 1,  4, 16, 64]) tensor([ 4, 25, 64, 16])
tensor([ 3,  7, 12, 12]) tensor([-1, -3, -4,  4]) tensor([ 1,  4, 16, 64]) tensor([ 4, 25, 64, 16]) tensor([ 2, 10, 32, 32]) tensor(76)
tensor([[12, 16],
        [48, 64]])
```





多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。



第三行分析

调用 torch.cat((X, Y), dim=0) 时，是沿着第一个维度（行）拼接这两个张量。这意味着 Y 的行会被添加到 X 的下方，因为第一个维度（dim=0）表示行。这样做的结果是一个形状为 (6, 4) 的新张量，其前三行来自 X，后三行来自 Y。

调用 torch.cat((X, Y), dim=1) 时，是沿着第二个维度（列）拼接这两个张量。这意味着 Y 的列会被添加到 X 的右侧，因为第二个维度（dim=1）表示列。结果是一个形状为 (3, 8) 的新张量，其每行前四个元素来自 X，后四个元素来自 Y。

```
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
print(torch.cat((X, Y), dim=0))
print(torch.cat((X, Y), dim=1))
```

我在想知道`dim=2`是怎么样的？

在第三行添加`dim=2`后出现了不存在，并给出以下解释

```
  torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1), torch.cat((X, Y), dim=2)
                                                        ^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)
```

发现x，y就是二维张量，不能进行沿第三维度进行拼

所以应该这么写，这么用:

想把这玩意扩展到三维张量，在进行维度上的拼接

```
# 首先，将二维张量扩展为三维张量，'unsqueeze'方法可以在指定位置添加一个维度
X_3d = X.unsqueeze(2)  # 这会将X变为一个形状为(3, 4, 1)的张量
Y_3d = Y.unsqueeze(2)  # 这会将Y变为一个形状为(3, 4, 1)的张量

# 现在可以在第三个维度（dim=2）上进行拼接
result_dim2 = torch.cat((X_3d, Y_3d), dim=2)

# 打印结果
print(result_dim2)

```





假如说现在想来看看布尔值，看看x和y的对应关系，可以这样

```
X == Y
```

```
tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
```



#### **广播机制**

是一种在不同形状的数组或张量之间执行逐元素操作（如加法、乘法等）的方法。在广播机制下，较小的数组会在必要时自动扩展到与较大数组相同的形状。

遵循以下规则：

- 扩展维度：如果两个数组的维度数不相同，那么较小维度数组的形状将会在前面被补1，直到与较大维度数组的维数相同。

- 扩展大小：在任意维度上，如果一个数组的大小为1（这个维度上只有一个元素），而另一个数组的大小大于1，那么首先会将第一个数组在这个维度上复制扩展，直到与另一个数组的大小相同。

- 兼容性：如果在所有维度上大小相等，或者一个数组在某个维度上的大小为1，则两个数组被认为是广播兼容的。

- 不兼容报错：如果在某个维度上两个数组的大小不一致且两者都不是1，则无法广播，会抛出错误。



```
X = torch.arange(12, dtype=torch.float32).reshape((4,3))
Y = torch.tensor([2.0, 1, 4])
X+Y
print(X+Y)
```

解：这个答案是通过广播机制得到的。在PyTorch中，当进行元素级操作时，如果两个张量的形状不匹配，PyTorch会尝试自动扩展它们的形状以匹配。

这里的张量`X`是一个形状为`(4, 3)`的二维张量，而张量`Y`是一个形状为`(3,)`的一维张量。当你尝试将`Y`加到`X`上时，PyTorch会将`Y`在第一个维度上复制四次以匹配`X`的形状，然后对应元素相加。具体步骤如下：

1. `X`的原始形状是`(4, 3)`:
  
   ```
   [[ 0.,  1.,  2.],
    [ 3.,  4.,  5.],
    [ 6.,  7.,  8.],
    [ 9., 10., 11.]]
   ```
   
2. `Y`在没有扩展之前的形状是`(3,)`:
   ```
   [2., 1., 4.]
   ```

3. `Y`扩展后的形状也是`(4, 3)`，通过复制其元素来匹配`X`的形状:
   ```
   [[2., 1., 4.],
    [2., 1., 4.],
    [2., 1., 4.],
    [2., 1., 4.]]
   ```

4. 然后，`X`和扩展后的`Y`对应元素相加得到最终结果：
   ```
   [[ 2.,  2.,  6.],
    [ 5.,  5.,  9.],
    [ 8.,  8., 12.],
    [11., 11., 15.]]
   ```
   
   

**scalar是tensor的子集**

是的，你可以这样理解。在PyTorch中，标量（scalar）可以被认为是一个零维的张量（tensor），即它没有任何维度。张量是一个更一般的概念，它可以是零维的（标量），一维的（向量），二维的（矩阵），或者更高维度。所以，所有的标量都可以被视作是形状为`()`的张量，这使得标量成为张量的一个特例。

#### 索引和切片

 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。

比如之前`x`是4*3的矩阵，在[-1]表示最后一行单独提出来，在[0]表示第一行单独提出来

这里都是**从0起步**

```
X[-1], X[1:3]
print(X[-1], X[1:3])
X[1, 2] = 110
print(X)
```

```
tensor([ 9., 10., 11.]) tensor([[3., 4., 5.],
        [6., 7., 8.]])
tensor([[  0.,   1.,   2.],
        [  3.,   4., 110.],
        [  6.,   7.,   8.],
        [  9.,  10.,  11.]])
```

#### 转化成其他对象

NumPy张量和tensor的区别

PyTorch张量（Tensor）和NumPy数组在很多方面都非常相似，它们都是多维数组，支持大量的相似操作，但也有一些关键的区别：

1. **计算框架**:
  
   - PyTorch是一个深度学习框架，其张量被设计为可以进行自动微分，非常适合于机器学习。
   - NumPy是一个用于科学计算的库，其数组不支持自动微分或神经网络构建等深度学习操作。
   
2. **GPU支持**:
   - PyTorch张量可以在GPU上进行操作，这对于深度学习应用是非常重要的，因为GPU可以显著加速计算。
   - NumPy数组默认只能在CPU上操作。

3. **自动微分**:
   - PyTorch提供了自动微分机制，通过计算图和梯度传播，支持复杂的机器学习模型训练。
   - NumPy没有内置的自动微分功能，不适合直接用于机器学习模型训练。

   两者提供的数学操作是等价的，只是在某些具体实现或性能优化上存在差异。对于专注于数学和科学计算的用户，NumPy提供了足够的功能。对于深度学习研究者和实践者，PyTorch提供了更加强大的工具和更高效的计算能力。

将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量

```
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```

要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数

```
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```





### 数据预处理

Python中常用的数据分析工具中，我们通常使用pandas软件包

**数据读取**

举一个例子，我们首先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 `../data/house_tiny.csv`中。 以其他格式存储的数据也可以通过类似的方式进行处理。 下面我们将数据集按行写入CSV文件中。

```
什么是csv文件？

CSV文件（Comma-Separated Values文件）是一种常用的数据文件格式，它以纯文本形式存储表格数据。CSV文件由多行组成，每行代表数据表中的一行，而每行中的每个单元格则通常由逗号分隔。CSV文件的简单结构使得它易于被人类阅读和编写，同时也容易被计算机程序解析和生成。
```

CSV文件的主要特点包括：

1. **简单性**：纯文本形式，可以使用文本编辑器打开。
2. **兼容性**：被广泛支持，大多数的表格软件、数据库和编程语言都能够处理CSV文件。
3. **灵活性**：没有固定的列数或数据类型要求，可以存储各种结构的数据。

CSV文件常用于数据导入导出操作，因为它们简单、兼容性好，并且格式开放。尽管“CSV”意味着使用逗号作为分隔符，但实际上也可以使用其他字符，如制表符（生成所谓的TSV文件）、分号等，这取决于数据的内容和所使用的地区标准。

比如例子中的：

```
import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # 列名
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

来自`chatgpt`的解释：

```
这段Python代码使用`os`和`os.path`库来创建一个新的目录，并在这个目录下写入一个名为`house_tiny.csv`的CSV文件。具体步骤如下：

1. `os.makedirs(os.path.join('..', 'data'), exist_ok=True)`: 这行代码创建一个名为`data`的新目录在当前工作目录的上一级目录中。如果这个目录已经存在，`exist_ok=True`参数将防止抛出错误。

2. `data_file = os.path.join('..', 'data', 'house_tiny.csv')`: 这行代码设置了新CSV文件的路径。

3. `with open(data_file, 'w') as f`：这行代码打开新的CSV文件以供写入。

4. 然后，使用`f.write()`函数写入列名和数据行：
   - `'NumRooms,Alley,Price\n'`: 列名，表示房间数量、小巷类型和价格。
   - `'NA,Pave,127500\n'`: 第一行数据，表示没有房间数量数据（NA），小巷类型为铺设（Pave），价格为127500。
   - `'2,NA,106000\n'`: 第二行数据，表示有2个房间，小巷类型数据缺失（NA），价格为106000。
   - `'4,NA,178100\n'`: 第三行数据，表示有4个房间，小巷类型数据缺失（NA），价格为178100。
   - `'NA,NA,140000\n'`: 第四行数据，表示房间数量和小巷类型数据都缺失（NA），价格为140000。

这个CSV文件现在包含了一个简单的房屋数据集，其中包括一些缺失值（NA）。这种格式的数据文件经常用于数据分析和机器学习任务。
```

要从创建的CSV文件中加载原始数据集，我们导入`pandas`包并调用`read_csv`函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）

```
# 如果没有安装pandas，只需取消对以下行的注释来安装pandas
# !pip install pandas
import pandas as pd

data = pd.read_csv(data_file)
print(data)
```



**处理缺失值**

这里的`NAN`代表的是缺失值

为了处理缺失的数据，典型的方法包括**插值法**和**删除法**

通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`， 其中前者为`data`的前两列，而后者为`data`的最后一列。 对于`inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。

```
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
```





## 数学方面

（感觉这一章之前都看过了，主要是练习`latex`）

### 线性代数

记录基础数学对象，运算和算术，并用数学知识和相应的代码实现它们

#### 标量向量

 严格来说，仅包含一个数值被称为*标量*（scalar）

```
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)

print（x + y, x * y, x / y, x**y）
```

向量就是标量值组成的列表，这些标量值被称为向量的*元素*（element）或*分量*（component）

```
x = torch.arange(4)
print(x)
```

在数学中，向量$x$可以写为
$$
\begin{split}
{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix}
\end{split}
$$

#### 张量：长度维度形状

不是特别理解`d2l`里面写的3点，根据解释和chat，我重新换个语言解释下这3个的意思

**长度：**

- 当讨论一个特定轴（axis）或维度（dimension）的长度时，我们指的是沿着那个轴的元素数量。

  例如，如果我们有一个向量 `[1, 2, 3]`，那么它的长度是3，因为它包含3个元素。

- 在多维数组（如矩阵或更高维度的张量）中，我们可能会说某一维度的长度是指在该维度上数组的大小。比如，在一个形状为 `(3, 5)` 的矩阵中，第一个维度的长度是3，第二个维度的长度是5。

**维度：**

- 维度通常指的是张量中轴的数量

  例如，一个向量是一个一维张量，一个矩阵是一个二维张量

- 维度也可以指向特定轴本身。比如，在三维张量中，我们可以讨论第一维、第二维或第三维。

**形状：**

- 形状是描述张量每个维度长度的一个术语，它是一个由整数组成的元组（tuple）。

  例如，一个形状为 `(3, 5, 7)` 的三维张量在第一个维度上长度为3，在第二个维度上长度为5，在第三个维度上长度为7。

  

#### 矩阵

矩阵，我们通常用粗体、大写字母来表示 ,比如：$\mathbf{X}$

$$
\begin{split}
\mathbf{A}=\begin{bmatrix}a_{11}&\ a_{12} &\cdots&\ a_{1n}
\\a_{21}&\ a_{22}&\cdots&\ a_{2n}\\ \vdots&\ \vdots&\ \ddots&\ \vdots\\a_{1m}&\ a_{2m}&\ \cdots&\ a_{mn}
\end{bmatrix}
\end{split}
$$


同上，代码之中是这样的：

```
A = torch.arange(20).reshape(-1, 4)
print(A)
```



**转置：**

$\mathbf{a}^\top$:将矩阵的行和列变换位置
$$
\begin{split}
\mathbf {A}^\top=\begin{bmatrix}a_{11}&\ a_{21}&\ \dots&\ a_{m1}\\
a_{12}&\ a_{22}&\ \cdots&\ a_{m2}\\ \vdots&\ \vdots&\ \ddots&\ \vdots \\a_{1n}&\ a_{2n}&\ \dots&\ a_{mn} 
\end{bmatrix}
\end{split}
$$

```
ptint(A.T)
```



#### **张量：**

开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现， 其中3个轴对应于高度、宽度，以及一个*通道*（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）

```
X = torch.arange(24).reshape(2, 3, 4)
print(X)
```



**张量的运算：**

加减：将两个相同形状的矩阵相加减，会在这两个矩阵上执行元素加法或者减法（前面提到过）

乘：2个矩阵按元素乘法称之为积*Hadamard*

比如说矩阵a和b的积：
$$
\begin{split}\mathbf
A\odot B=\begin{bmatrix}
a_{11}b_{11}&\ a_{12}b_{12}&\ \dots
&\ a_{1n}b_{1n}\\a_{21}b_{21}&\ a_{22}b_{22}&\ \dots&\ a_{2n}b_{2n}\\ \vdots&\ \vdots&\ \ddots&\ \vdots\\ a_{m1}b_{m1}&\ a_{m2}b_{m2}&\ \dots&\ a_{mn}b_{mn}
\end{bmatrix}
\end{split}
$$
就是单纯对应数字相乘



**降维：**

我们可以表示任意形状张量的元素和。 例如，矩阵$A$中元素的和可以记为$\sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}$

```
A.shape, A.sum()
```

**调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量**

我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。



#### 点积：

$$
\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i
$$

```
x = torch.arange(4, dtype=torch.float32)
# 然后定义了一个张量 y
y = torch.ones(4, dtype = torch.float32)
# 点积就是把它们对应相乘相加
print(x, y, torch.dot(x, y))
```



#### 矩阵-向量积：

矩阵$A$用它的行向量表示：
$$
\begin{split}\mathbf{A}=\begin{bmatrix}
\mathbf{a}^\top_{1} \\ \mathbf{a}^\top_{2}
\\
\vdots
\\
\mathbf{a}^\top_{m}
\end{bmatrix}
\end{split}
$$
矩阵向量积$\mathbf{A}\mathbf{x}$是一个长度为$m$的列向量， 其第$i$个元素是点积$a_{i}^\top x$：
$$
\begin{split}
\mathbf{A}\mathbf{x}=\begin{bmatrix}
\mathbf{a_{1}^\top}
\\
\mathbf{a_{2}^\top}
\\
\vdots
\\
\mathbf{a_{m}^\top}
\end{bmatrix}
\mathbf{x}=
\begin{bmatrix}
\mathbf{a_{1}^\top}\mathbf{x}
\\
\mathbf{a_{2}^\top}\mathbf{x}
\\
\vdots
\\
\mathbf{a_{m}^\top}\mathbf{x}
\end{bmatrix}
\end{split}
$$


#### 矩阵-矩阵乘法：

$$
\begin{split}
\mathbf{A}=\begin{bmatrix}
a_{11}&\ a_{12}&\ \dots&\ a_{1k}
\\
a_{21}&\ a_{22}&\ \dots&\ a_{2k}
\\
\vdots&\ \vdots&\ \ddots&\ \vdots
\\
a_{n1}&\ a_{n2}&\ \dots&\ a_{nk}
\end{bmatrix}
\end{split}
,
\begin{split}
\mathbf{b}=\begin{bmatrix}
b_{11}&\ b_{12}&\ \dots&\ b_{1k}
\\
b_{21}&\ b_{22}&\ \dots&\ b_{2k}
\\
\vdots&\ \vdots&\ \ddots&\ \vdots
\\
b_{n1}&\ b_{n2}&\ \dots&\ b_{nk}
\end{bmatrix}
\end{split}
$$

生成矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$就是简单的将简单地将每个元素$c_{ij}$计算为点积$a_i^\top b_j$
$$
\begin{split}\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.\end{split}
$$
代码直接表示就好了：

```
B = torch.ones(4, 3)
torch.mm(A, B)
```



刚才打开发现没有写梯度：

#### 梯度

